{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오픈소스 머신러닝 라이브러리\n",
    "머신러닝(Machine Learning) 모델을 구현시키고, 훈련시키고, 훈련된 모델을 통해 예측해볼 수 있음\n",
    "Ex) TensorFlow, PyTorch, Scikit-Learn, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아나콘다 환경 활성화 - 터미널에서 진행\n",
    "- conda info --envs #아나콘다 가상환경 리스트 확인\n",
    "- conda activate 가상환경이름 #아나콘다 가상환경 활성화\n",
    "- conda install sklearn #사이킷런 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사이킷런 라이브러리 불러오기\n",
    "- 아나콘다 환경 설정 후, 아나콘다에 sklearn 설치 후 임포트 가능\n",
    "- conda install sklearn # 아나콘다에서 사이킷런 설치 명령어\n",
    "- [sklearn 모델 종류 보러가기] (https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTree(의사결정나무)란?\n",
    "- 개념\n",
    "    * 의사결정 규칙, 즉 특정 질문에 따라 그 결과들을 트리 구조로 도식화한 의사 결정 지원 도구의 일종\n",
    "    * 한쪽 방향으로 쏠리도록 해주는 특정 값을 찾고, 이를 불순도를 기반으로 찾아냄 -> Leaf Node가 가장 섞이지 않은 상태로 완전히 분류되는 것, 즉 복잡성(entropy)이 낮도록 만드는 것\n",
    "- 원리\n",
    "    * 각 데이터의 행들은 컬럼을 가지고 있는데, 해당 컬럼을 기준으로 특정한 값을 정하면, 이를 기준으로 모든 행을 두개의 노드(node)로 분류(Binary decision rule, 이진분할) 할 수 있음. 그렇게 되면, 2개의 노드가 파생되는데 앞의 과정을 반복해 또 다시 새로운 피쳐의 특정 값을 정하고 분류를 진행함. 이 과정을 반복하면 점차 피쳐의 값에 따라 데이터들이 분류됨\n",
    "- 장점\n",
    "    1. 주요 변수와 분리기준을 제시하기 때문에, 직관적인 해석이 가능해 결과 해석 용이\n",
    "    2. 비모수적 모델 : 통계모델에 요구되는 가정에 자유로움\n",
    "        - 비모수 : 연속되지 않은 값 #데이터가 정규분포가 아니며 데이터의 표본 수가 적거나 부족하고 데이터가 서로 독립적인 경우\n",
    "    3. 변수간 상호작용을 고려해 선형/비선형 관계 탐색 가능\n",
    "- 단점\n",
    "    1. 적은 데이터 수 등 여러 요인에 대해 Overfitting(과적합)될 확률이 높기 때문에 가지치기 필요\n",
    "    2. 전체적인 선형관계의 파악이 미흡함\n",
    "    3. 비연속성 : 분리 시 연속형 변수를 구간화하기 때문에 분리 경계점 근처에 오류 발생 가능\n",
    "- 불순도 (Impurity)\n",
    "    - 복잡성을 의미하고, 해당 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지를 뜻함.\n",
    "    - 따라서, 다양한 데이터가 섞여 있을수록 불순도 ⬆\n",
    "- ex) CART 의사결정 나무(Classification And Regression Tree) : CART는 가장 널리 사용되는 의사결정나무 알고리즘으로, 이름에서도 알 수 있듯이 분류와 회귀 나무에서 모두 사용할 수 있음. 불순도를 측정할 때 목표 변수(y)가 범주형인 경우 지니 지수를 사용하고, 연속형인 경우 분산을 사용하여 이진 분할을 함. \n",
    "- 모델 선언 방법\n",
    "    ```\n",
    "    import sklearn #sklearn 라이브러리 가져오기\n",
    "    from sklearn.tree import DecisionTreeRegressor # sklearn의 의사결정나무(회귀) 모델 가져오기 \n",
    "    ds_model = DecisionTreeRegressor() #의사결정나무 모델 선언\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest(랜덤포레스트)란?\n",
    "- 개념\n",
    "    * 분류, 회귀 분석 등에 사용되는 앙상블 학습 방법의 일종으로, 훈련 과정에서 구성한 **여러 개의 Decision Tree(의사결정나무)**로 부터 이들의 **최빈값** 또는 **평균 예측 값**을 통해 예측을 결정하여, 예측의 성능을 높이는 방법\n",
    "    * 의사결정나무 모델만을 사용한 배깅 모델의 일종\n",
    "- 장점\n",
    "    1. 일반화 및 성능 우수\n",
    "    2. 파라미터 조정 용이\n",
    "    3. 데이터 스케일링 변화 불필요\n",
    "    4. Overfitting(과적합) 잘 되지 않음\n",
    "- 단점\n",
    "    1. 개별 트리 분석이 어렵고, 트리 분리가 복잡해짐\n",
    "    2. 차원이 크고, 희소한 데이터는 성능이 미흡함\n",
    "    3. 훈련시 메모리 소모가 큼\n",
    "    4. 훈련용 데이터를 추가해도 모델 성능을 개선하기 어려움\n",
    "- 앙상블(Ensemble) 기법 : \n",
    "    * 여러 개의 분류기를 통해 도출된 예측 결과를 결합하여 보다 정확하고 신뢰성이 높은 예측값을 도출하는 기법  \n",
    "    * 주어진 하나의 데이터로부터 여러 개의 랜덤 데이터셋을 추출하여, 각 데이터셋을 통해 여러개의 모델을 생성함\n",
    "    * 종류 :  보팅(Voting), 배깅(Bagging), 부스팅(Boosting)\n",
    "        * 보팅(Voting) : 각각 **다른** 알고리즘을 이용한 분류기를 결합하는 방식으로 최종 예측 값을 투표하는 방식으로 이루어짐\n",
    "            * 하드 보팅(Hard Voting) : **다수의 분류기가 결정한 예측값**을 최종 보팅 결과값으로 선정 (다수결)\n",
    "            * 소프트 보팅(Soft Voting) : 분류기들의 레이블 값 결정 확률을 모두 더하고, **평균 내 확률**이 가장 높은 레이블 값을 최종 보팅 결과값으로 선정\n",
    "        * 배깅(Bagging) : 서로 **같은** 알고리즘을 이용한 분류기가 **데이터 샘플링**을 서로 다르게 가져가면서 학습을 수행\n",
    "        * 부스팅(Boosting) : 여러 개의 분류기가 순차적으로 학습을 진행하는데 앞에서 학습한 분류기가 예측이 틀린 데이터에 관해서 다음 분류기에게 가중치(Weight)를 부여하면서 학습과 예측을 진행하는 방식\n",
    "- 평가지표 (RandomForestRegressor(criterion='평가척도'))\n",
    "    * 평가 지표는 **실제 값과 예측 값의 차이**를 기반으로 하기 때문에, **값이 작을수록** **성능 ⬆️**\n",
    "    * 평가 지표 예시\n",
    "        * MAE (Mean Absolue Error) : 실제 값과 예측 값의 차이를 **절댓값**으로 변환해 평균한 것\n",
    "        * MSE (Mean Squared Error) : 실제 값과 예측 값의 차이를 **제곱해 평균**한 것\n",
    "        * RMSE (Root Mean Squared Error) : MSE 값은 오류의 제곱을 구하므로, **실제 오류 평균보다 더 커지는 특성**이 있어 **MSE에 루트**를 씌운 RMSE 값을 사용함\n",
    "- 변수 중요도 파악\n",
    "    * 변수 중요도\n",
    "        * 예측 변수를 결정할 때 각 피쳐(컬럼)가 얼마나 중요한 역할을 하는지에 관한 척도\n",
    "        * 변수 중요도가 낮다면 해당 피쳐를 제거(`데이터프레임.drop(['변수중요도⬇ 피처'], axis=1)`)해서 모델 성능 높이기\n",
    "    * feature_importances_ 속성 : fit()으로 훈련이 완료된 모델에 관하여 변수 중요도 파악 가능\n",
    "- 모델 사용 방법\n",
    "    ```\n",
    "    import sklearn #sklearn 라이브러리 가져오기\n",
    "    from sklearn.ensemble import RandomForestRegressor # sklearn의 랜덤포레스트(회귀) 모델 가져오기 \n",
    "    rf_model = RandomForestRegressor(criterion='mse') #평가지표를 MSE로하는 랜덤포레스트 모델 선언\n",
    "    rf_model.fit(학습데이터) #학습데이터를 이용해 랜덤포레스트 모델 학습\n",
    "    rf_model.feature_importances_ #변수 중요도 파악\n",
    "\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼 파라미터\n",
    "- 하이퍼 파라미터\n",
    "    * 모델링할 때 사용자가 직접 세팅해주는 값\n",
    "    * Ex ) learning rate나 트리 기반 모델의 트리의 최대 깊이, 최소 노드 갯수 등\n",
    "    * 하이퍼 파라미터 튜닝 : 하이퍼 파라미터 값들을 세팅해주는 행위\n",
    "- 의사결정 나무의 하이퍼 파라미터 튜닝\n",
    "    * 정지 규칙을 설정해주는 것\n",
    "        * 정지 규칙: 더 이상 트리의 분리(가지치기)가 일어나지 않도록 하는 규칙\n",
    "        * 가지 치기 : 의사결정나무의 과적합을 줄이고 일반화 가능성을 증대시키는 sub-tree를 찾는 과정\n",
    "    * 정지 규칙 종류\n",
    "        1. 최대깊이 (max_depth) : 최대로 내려갈 수 있는 depth 로서, 뿌리 노드로부터 내려갈 수 있는 깊이를 지정하며 작을수록 트리는 작아짐\n",
    "        2. 최소 노드크기(min_samples_split) : **노드를 분할하기 위한 데이터 수**로서, 해당 노드에 이 값보다 적은 확률변수 수가 있다면 멈추고, 작을수록 트리는 커짐\n",
    "        3. 최소 향상도(min_impurity_decrease) : **노드를 분할하기 위한 최소 향상도**로서, 향상도가 설정값 이하라면 더 분할하지 않음. 작을수록 트리는 커짐\n",
    "        4. 비용복잡도(Cost-complexity) : 트리가 커지는 것에 대해 **패널티 계수를 설정**해서 불순도와 트리가 커지는 것에 대해 **복잡도를 계산**하는 것\n",
    "    * 하이퍼 파라미터 튜닝 예시 (GridSearch)\n",
    "        * 개념 : 완전 탐색(Exhaustive Search) 방식을 사용하고, 가능한 모든 조합 중에서 가장 우수한 조합을 찾아줌\n",
    "        * 장점 : 가능한 모든 조합 중에서 가장 우수한 조합을 찾아줌\n",
    "        * 단점 : 시간이 매우 오래 걸림\n",
    "        * 사용 방법 :  \n",
    "            ```\n",
    "            from sklearn.model_selection import GridSearchCV #그리드서치 모델 불러오기\n",
    "            model = RandomForestRegressor(criterion = 'mse', random_state=2020) #랜덤포레스트 모델 선언\n",
    "            params = {'n_estimators': [200, 300, 500],\n",
    "            'max_features': [5, 6, 8],\n",
    "            'min_samples_leaf': [1, 3, 5]} #n_estimators: 나무 갯수, max_features: 몇개의 컬럼을 선택할지, #min_sample_leaf: 마지막 노드의 최대 개수\n",
    "            greedy_CV = GridSearchCV(model, param_grid=params, cv = 3, n_jobs = -1) #그리드서치 모델 생성\n",
    "            greedy_CV.fit(X_train, Y_train) #학습시키기\n",
    "            ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델훈련(의사결정나무)\n",
    "1. 라이브러리 불러오기\n",
    "- \n",
    "    ```\n",
    "    import pandas as pd # 머신러닝에 데이터를 적용하기위해 필요한 라이브러리\n",
    "    import sklearn #sklearn 라이브러리 가져오기\n",
    "    from sklearn.tree import DecisionTreeRegressor # sklearn의 의사결정나무 모델 가져오기 \n",
    "    ``` \n",
    "\n",
    "2. 데이터 불러오기\n",
    "- csv 파일을 데이터프레임 객체로 불러오기\n",
    "    ```\n",
    "    train = pd.read_csv('data/train.csv') \n",
    "    test = pd.read_csv('data/test.csv') \n",
    "    ```\n",
    "\n",
    "3. 데이터 전처리 - 결측치 제거 & 확인하기\n",
    "    ``` \n",
    "    test = test.dropna() # 결측치 보유 행 제거하기\n",
    "    train = train.dropna()\n",
    "    print(train.isnull().sum()) # 결측치 개수 확인하기\n",
    "    ```\n",
    "\n",
    "4. 데이터 전처리 - 예측 데이터(X_train), 레이블 데이터(Y_train) 분리하기\n",
    "- train 데이터중 예측해야할 count 피쳐를 drop()함수를 사용하고, axis=1 옵션을 사용해 해당 열을 제외한 데이터들을 X_train 이라는 이름의 DataFrame 객체로 만들고\n",
    "count 피쳐만을 데이터로 갖는 Y_train 이라는 이름의 DataFrame 객체로 만들기\n",
    "    ```\n",
    "    X_train = train.drop(['제외할컬럼명'], axis=1)\n",
    "    Y_train = train['예측할컬럼명']\n",
    "    ```\n",
    "\n",
    "5. 모델 훈련시키기 : model.fit(이용피처데이터, 예측라벨데이터)\n",
    "- model 변수명으로 모델을 선언하고, fit() 함수를 사용해서, X_train 을 input 으로 삼고, Y_train 을 output 으로 삼아 모델을 훈련시킴\n",
    "    ```\n",
    "    model = DecisionTreeRegressor() #모델 선언\n",
    "    model.fit(X_train, Y_train) # fit() : 모델 훈련\n",
    "    ```\n",
    "- Ex)\n",
    "    ```\n",
    "    X_train = train.drop(['count'], axis=1) #count 변수의 행 모두 지우고, 데이터프레임 객체로 만들기\n",
    "    Y_train = train['count'] #count 변수만 남기고, 데이터프레임 객체로 만들기\n",
    "    ```\n",
    "\n",
    "6. 훈련된 모델을 사용해 테스트 파일 예측하기 : model.predict(예측하고싶은데이터)\n",
    "- 훈련된 모델에서 predict() 메소드를 통해 예측하고자하는 데이터를 인자로 넣어주면, 해당 데이터에 관한 예측 결과가 array 형태로 리턴됨\n",
    "    ```\n",
    "    [할당할 array] = model.predict(예측하고싶은데이터)\n",
    "    ``` \n",
    "- Ex)\n",
    "    ```\n",
    "    pred = model.predict(test)\n",
    "    pred[:5] #리스트에서 상위 다섯개 요소 확인하기\n",
    "    >>> array([117., 164.,  88.,  49.,  64.])\n",
    "    ```\n",
    "- 참고 : 상위 다섯개 확인하기 (데이터프레임 & 넘파이 배열)\n",
    "    - 데이터프레임 : `데이터프레임.head()`\n",
    "    - 넘파이 배열 : `넘파이배열[:5]` #파이썬 슬라이싱\n",
    "\n",
    "7. Dacon 제출 파일 생성 - to_csv() : csv 파일로 변환하기\n",
    "- to_csv() : 데이터프레임 객체에 저장된 내용을 csv파일로 내보내기\n",
    "    ```\n",
    "    submission = pd.read_csv('data/submission.csv') # submission.csv 파일 df 파일로 불러오기 : 제출하고자 하는 파일을 위해 생성한 빈 csv 파일을 데이터프레임 객체로 불러오기\n",
    "    submission['count'] = pred # submission df 파일의 count 피쳐에 예측결과 할당하기 : 해당 데이터프레임에 정답 파일을 csv 파일로 내보낸 후,\n",
    "    submission.to_csv('sub.csv',index=False) # 제출파일 생성하기 : 드라이브에 저장된 csv 파일을 다운받아서 제출하기\n",
    "    ```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc836340034460e9c5583b996b053fe1fe7ffeb52054821d3ff9d502e04de020"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('woodeem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
